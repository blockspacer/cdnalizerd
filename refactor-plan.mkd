# Worker management

Jobs will come from two sources:

 * The mainProcess (file change watcher)
 * Start up (where we sync everything)

There are two resources that we want to protect:

 * Connection overhead - we want to reuse current connections
 * Delay after uploading a file - with cloud files after uploading there's a 1.5 or second delay before the API server returns 200 OK 

As a sysadmin we care about:

 * Not flooding the network - Currently we just set max uploaders, we could later on set a max upload speed
 * Not flooding disk IO - we may have to deal with big files and uploading in chunks
 * Not flooding CPU - shouldn't be a problem

Version 1 plan:

 * A global uploader manager will hold
   + map of username -> uploader worker list
   + map of username -> jobsToDo
 * When a new job is added, it will check:
   + If there are zero upload workers for that URL, launch one
   + If there are any upload workers for that URL that are idle, (sleeping after uploading), wake one up
   + If there are max_workers_per_url already open and busy, just add it to their queue
   + If there are max_upload_workers busy, and there is no worker for that url, start at least one, then add it to the queue

Later improvements:

 * Configurable options for max_upload_workers and max_workers_per_url
 * Bandwidth limiting
 * Smarter worker counting, maybe maybe worker size should be 50% of queue_size + 1

# Models

## Stuff loaded from the config file

 * Endpoint - loaded from the config file
   + name - just a label you give it; will appear in the logs
   + username
   + apikey
   + region - can be ord/dfw/iad/hkd/syd/etc..
   + snet (wether to use the servicenet url)

 * File Pair - loaded from config file
   + Endpoint
   + move/copy - should we delete after uploading the file
   + source-path
   + destination-container
   + destination-prefix

## Stuff loaded after startup

 * Endpoint Details - loaded after logging in through the RS API
   + token - Access token
   + token-expiry - date when it expires
   + json (contains connection info for each service (eg. cloud files) and (region) and servicenaet
   + DFW-url
   + DFW-snet-url
   + HKG-url
   + HKG-snet-url
   + IAD-url
   + IAD-snet-url
   + ORD-url
   + ORD-snet-url
   + SYD-url
   + SYD-snet-url

All of the above is loaded into a single tree.

## Stuff used when live

 * Job
   + Source file 
   + Destination prefix

# Operations - These are business level library functions

## Load config file

 * Loads the config file and returns the entries

## Fill in Endpoint details

 * Takes the JSON from the RS login info and returns an Endpoint Details

# Processes - These are long running processes (cooperative (non preemptive) threads)

## Loader

 * Goes through the config
 * Starts sub workers to load the Endpoint Details for each endpoint
 * Waits for those to finish
 * Starts file watcher workers - one for each file pair entry in the config
 * Ends

## File watcher

 * Takes Endpoint - Endpoint Details - File Pair
 * Sets up inotify watches for the dir and its sub dirs
 * Waits for a file to come in
 * Manages uploading
   + Holds a list of upload workers
   + Holds a queue of work to do, which the upload workers read from
   + 1 job = 1 worker
   + 2 jobs = 2 workers
   + 3 jobs = 2 workers
   + 4 jobs = 2 workers
   + 5+ jobs = 3 workers

## Upload worker

 * Input:
   + Url to connect to
   + Access token
   + move/copy
   + destination container
   + destination prefix
   + job queue to read from
 * Reads from a queue of files to process
 * Connects to Rackspace
 * Uploads all the files in the job queue
 * Sleeps one second in case more jobs come in
 * Disconnects and dies
